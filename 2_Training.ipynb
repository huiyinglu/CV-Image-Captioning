{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train our CNN-RNN model.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, we will customize the training of our CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values we set now will be used when training our model in **Step 2** below.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.\n",
    "- `save_every` - determines how often to save the model weights.  We will set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "## Explanations to how do we construct our model?\n",
    "### 1 CNN-RNN architecture\n",
    "\n",
    "**explanations:** \n",
    "Regarding the encoder CNN network - it's a pre-trained ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images.  The output is then flattened to a vector, before being passed through a `Linear` layer to transform the feature vector to have the same size as the word embedding.\n",
    "\n",
    "The decoder RNN network starts with a word embedding layer which takes the embedded image feature vector from encoder CNN network, followed by an LSTM layer, then Linear layer which generates the predicted score for the output caption word. The value for batch_size is 128 which is power of 2, and is one of the popular batch_size (32, 64, 128). The vocabulary threshold is 4, which we've tried in the previous notebook and seem to be a reasonable number to remove infrequent words. The embedded dimension is 256, which is between 100-300 as described in these 2 papers https://nlp.stanford.edu/pubs/glove.pdf and https://arxiv.org/pdf/1310.4546.pdf. The hidden layer size is 512, which is one of the suggested hidden layer size in many papers.\n",
    "\n",
    "### 2 Transform in transform_train\n",
    "\n",
    "**explanations:**\n",
    "I resize the image to 256 since some image might be smaller than 224. RandomCrop with 224 is necessary to ensure the size of images to be the same size and is the same size used in pre-trained CNN. RandomHorizontalFlip with 0.5 probability auguments training image data so that the model can be trained on more dataset. Normalize the images with the same average and standard deviation value as the pre-trained model's distribution.\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, we will specify a Python list containing the learnable parameters of the model.  For instance, if we decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then we should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### 3 Selection of trainable parameters\n",
    "\n",
    "**explanations:** The trainable parameters I select are: params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "The reason why I select only encoder.embed.parameters is because we use pre-trained ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images.  The output is then flattened to a vector, before being passed through a `Linear` layer to transform the feature vector to have the same size as the word embedding. So only last layer (encoder.embed) from the encoder network has trainable parameters. \n",
    "\n",
    "For decoder network, since all layers has trainable parameters, so I select all of those.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, we will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### 4 Selection of the optimizer\n",
    "\n",
    "**explanations:** I use Adam as the optimizer. The reason why I chose Adam (adaptive moment estimation, which uses past gradients to calculate current gradients) are: it's pretty widespread, and is practically accepted for use in training neural nets. On average, it performs better than other optimizer on CNN/RNN Neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.90s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1211/414113 [00:00<01:07, 6073.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:09<00:00, 5921.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## Select appropriate values for the Python variables below.\n",
    "batch_size = 128           # batch size\n",
    "vocab_threshold = 4        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train Our Model\n",
    "\n",
    "It will be useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that we will like to load are (`encoder_file` and `decoder_file`).  Then we can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/3236], Loss: 3.8191, Perplexity: 45.5628\n",
      "Epoch [1/3], Step [200/3236], Loss: 3.5648, Perplexity: 35.3333\n",
      "Epoch [1/3], Step [300/3236], Loss: 3.3685, Perplexity: 29.0356\n",
      "Epoch [1/3], Step [400/3236], Loss: 3.2900, Perplexity: 26.8423\n",
      "Epoch [1/3], Step [500/3236], Loss: 3.4279, Perplexity: 30.8123\n",
      "Epoch [1/3], Step [600/3236], Loss: 3.0085, Perplexity: 20.2577\n",
      "Epoch [1/3], Step [700/3236], Loss: 2.8884, Perplexity: 17.9644\n",
      "Epoch [1/3], Step [800/3236], Loss: 2.8039, Perplexity: 16.5089\n",
      "Epoch [1/3], Step [900/3236], Loss: 2.7008, Perplexity: 14.8920\n",
      "Epoch [1/3], Step [1000/3236], Loss: 4.0320, Perplexity: 56.3708\n",
      "Epoch [1/3], Step [1100/3236], Loss: 2.6640, Perplexity: 14.3538\n",
      "Epoch [1/3], Step [1200/3236], Loss: 2.6839, Perplexity: 14.6420\n",
      "Epoch [1/3], Step [1300/3236], Loss: 2.4073, Perplexity: 11.1040\n",
      "Epoch [1/3], Step [1400/3236], Loss: 2.3821, Perplexity: 10.8276\n",
      "Epoch [1/3], Step [1500/3236], Loss: 2.3732, Perplexity: 10.7322\n",
      "Epoch [1/3], Step [1600/3236], Loss: 2.3670, Perplexity: 10.6652\n",
      "Epoch [1/3], Step [1700/3236], Loss: 2.5152, Perplexity: 12.3685\n",
      "Epoch [1/3], Step [1800/3236], Loss: 2.4243, Perplexity: 11.2946\n",
      "Epoch [1/3], Step [1900/3236], Loss: 2.3765, Perplexity: 10.7671\n",
      "Epoch [1/3], Step [2000/3236], Loss: 2.2806, Perplexity: 9.78271\n",
      "Epoch [1/3], Step [2100/3236], Loss: 2.4518, Perplexity: 11.6097\n",
      "Epoch [1/3], Step [2200/3236], Loss: 2.2867, Perplexity: 9.84268\n",
      "Epoch [1/3], Step [2300/3236], Loss: 2.2677, Perplexity: 9.65696\n",
      "Epoch [1/3], Step [2400/3236], Loss: 2.5751, Perplexity: 13.1321\n",
      "Epoch [1/3], Step [2500/3236], Loss: 2.3901, Perplexity: 10.9141\n",
      "Epoch [1/3], Step [2600/3236], Loss: 2.2681, Perplexity: 9.66145\n",
      "Epoch [1/3], Step [2700/3236], Loss: 2.4130, Perplexity: 11.1677\n",
      "Epoch [1/3], Step [2800/3236], Loss: 2.1587, Perplexity: 8.66021\n",
      "Epoch [1/3], Step [2900/3236], Loss: 2.2137, Perplexity: 9.14927\n",
      "Epoch [1/3], Step [3000/3236], Loss: 2.2722, Perplexity: 9.70067\n",
      "Epoch [1/3], Step [3100/3236], Loss: 2.0950, Perplexity: 8.12556\n",
      "Epoch [1/3], Step [3200/3236], Loss: 2.6429, Perplexity: 14.0540\n",
      "Epoch [2/3], Step [100/3236], Loss: 2.5073, Perplexity: 12.27163\n",
      "Epoch [2/3], Step [200/3236], Loss: 2.1519, Perplexity: 8.60083\n",
      "Epoch [2/3], Step [300/3236], Loss: 2.0509, Perplexity: 7.775138\n",
      "Epoch [2/3], Step [400/3236], Loss: 2.5795, Perplexity: 13.1904\n",
      "Epoch [2/3], Step [500/3236], Loss: 2.0575, Perplexity: 7.82637\n",
      "Epoch [2/3], Step [600/3236], Loss: 2.1841, Perplexity: 8.88288\n",
      "Epoch [2/3], Step [700/3236], Loss: 2.1569, Perplexity: 8.64445\n",
      "Epoch [2/3], Step [800/3236], Loss: 2.0566, Perplexity: 7.81924\n",
      "Epoch [2/3], Step [900/3236], Loss: 2.1409, Perplexity: 8.50707\n",
      "Epoch [2/3], Step [1000/3236], Loss: 2.1933, Perplexity: 8.9648\n",
      "Epoch [2/3], Step [1100/3236], Loss: 2.0275, Perplexity: 7.59516\n",
      "Epoch [2/3], Step [1200/3236], Loss: 2.1119, Perplexity: 8.26364\n",
      "Epoch [2/3], Step [1300/3236], Loss: 2.0162, Perplexity: 7.50966\n",
      "Epoch [2/3], Step [1400/3236], Loss: 2.6954, Perplexity: 14.8109\n",
      "Epoch [2/3], Step [1500/3236], Loss: 2.3037, Perplexity: 10.0113\n",
      "Epoch [2/3], Step [1600/3236], Loss: 2.0749, Perplexity: 7.96358\n",
      "Epoch [2/3], Step [1700/3236], Loss: 2.0503, Perplexity: 7.77012\n",
      "Epoch [2/3], Step [1800/3236], Loss: 2.0769, Perplexity: 7.97959\n",
      "Epoch [2/3], Step [1900/3236], Loss: 2.3362, Perplexity: 10.3416\n",
      "Epoch [2/3], Step [2000/3236], Loss: 2.5521, Perplexity: 12.8340\n",
      "Epoch [2/3], Step [2100/3236], Loss: 2.0410, Perplexity: 7.69814\n",
      "Epoch [2/3], Step [2200/3236], Loss: 2.0359, Perplexity: 7.65950\n",
      "Epoch [2/3], Step [2300/3236], Loss: 2.2564, Perplexity: 9.54894\n",
      "Epoch [2/3], Step [2400/3236], Loss: 3.0331, Perplexity: 20.7623\n",
      "Epoch [2/3], Step [2500/3236], Loss: 2.0263, Perplexity: 7.58583\n",
      "Epoch [2/3], Step [2600/3236], Loss: 2.2781, Perplexity: 9.75860\n",
      "Epoch [2/3], Step [2700/3236], Loss: 1.9411, Perplexity: 6.96649\n",
      "Epoch [2/3], Step [2800/3236], Loss: 2.1018, Perplexity: 8.18078\n",
      "Epoch [2/3], Step [2900/3236], Loss: 2.1996, Perplexity: 9.02152\n",
      "Epoch [2/3], Step [3000/3236], Loss: 1.9428, Perplexity: 6.97817\n",
      "Epoch [2/3], Step [3100/3236], Loss: 2.1574, Perplexity: 8.64902\n",
      "Epoch [2/3], Step [3200/3236], Loss: 2.1268, Perplexity: 8.38776\n",
      "Epoch [3/3], Step [100/3236], Loss: 2.6140, Perplexity: 13.65396\n",
      "Epoch [3/3], Step [200/3236], Loss: 2.0475, Perplexity: 7.74876\n",
      "Epoch [3/3], Step [300/3236], Loss: 1.9807, Perplexity: 7.24750\n",
      "Epoch [3/3], Step [400/3236], Loss: 2.1100, Perplexity: 8.24810\n",
      "Epoch [3/3], Step [500/3236], Loss: 2.1956, Perplexity: 8.98550\n",
      "Epoch [3/3], Step [600/3236], Loss: 2.3213, Perplexity: 10.1893\n",
      "Epoch [3/3], Step [700/3236], Loss: 1.9526, Perplexity: 7.04711\n",
      "Epoch [3/3], Step [800/3236], Loss: 1.9609, Perplexity: 7.10552\n",
      "Epoch [3/3], Step [900/3236], Loss: 1.9155, Perplexity: 6.79026\n",
      "Epoch [3/3], Step [1000/3236], Loss: 2.2893, Perplexity: 9.8678\n",
      "Epoch [3/3], Step [1100/3236], Loss: 1.9563, Perplexity: 7.07296\n",
      "Epoch [3/3], Step [1200/3236], Loss: 2.0442, Perplexity: 7.72284\n",
      "Epoch [3/3], Step [1300/3236], Loss: 1.8663, Perplexity: 6.46469\n",
      "Epoch [3/3], Step [1400/3236], Loss: 2.0868, Perplexity: 8.05893\n",
      "Epoch [3/3], Step [1500/3236], Loss: 2.1471, Perplexity: 8.56001\n",
      "Epoch [3/3], Step [1600/3236], Loss: 1.8737, Perplexity: 6.51244\n",
      "Epoch [3/3], Step [1700/3236], Loss: 1.9162, Perplexity: 6.79547\n",
      "Epoch [3/3], Step [1800/3236], Loss: 1.8206, Perplexity: 6.17581\n",
      "Epoch [3/3], Step [1900/3236], Loss: 1.9158, Perplexity: 6.79219\n",
      "Epoch [3/3], Step [2000/3236], Loss: 2.0385, Perplexity: 7.67921\n",
      "Epoch [3/3], Step [2100/3236], Loss: 2.0008, Perplexity: 7.39505\n",
      "Epoch [3/3], Step [2200/3236], Loss: 1.9099, Perplexity: 6.75235\n",
      "Epoch [3/3], Step [2300/3236], Loss: 1.9152, Perplexity: 6.78810\n",
      "Epoch [3/3], Step [2400/3236], Loss: 1.9084, Perplexity: 6.74241\n",
      "Epoch [3/3], Step [2500/3236], Loss: 1.9040, Perplexity: 6.71261\n",
      "Epoch [3/3], Step [2600/3236], Loss: 1.9369, Perplexity: 6.93717\n",
      "Epoch [3/3], Step [2700/3236], Loss: 1.7671, Perplexity: 5.85415\n",
      "Epoch [3/3], Step [2800/3236], Loss: 1.8840, Perplexity: 6.58016\n",
      "Epoch [3/3], Step [2900/3236], Loss: 1.8629, Perplexity: 6.44249\n",
      "Epoch [3/3], Step [3000/3236], Loss: 2.0395, Perplexity: 7.68681\n",
      "Epoch [3/3], Step [3100/3236], Loss: 1.9950, Perplexity: 7.35204\n",
      "Epoch [3/3], Step [3200/3236], Loss: 1.9255, Perplexity: 6.85836\n",
      "Epoch [3/3], Step [3236/3236], Loss: 2.0495, Perplexity: 7.76446"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
